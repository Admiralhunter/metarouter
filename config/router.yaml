server:
  host: "0.0.0.0"
  port: 8000

lm_studio:
  # Single instance (backward compatible):
  base_url: "http://localhost:1234"
  timeout: 300
  refresh_interval: 60  # Per-instance model list cache TTL (seconds)
  health_check_interval: 30  # Background health check frequency (seconds)

  # Multi-instance: uncomment and configure to use multiple LM Studio machines.
  # When 'instances' is set, the single base_url above is ignored.
  # instances:
  #   - name: "desktop"
  #     base_url: "http://localhost:1234"
  #     timeout: 300
  #     refresh_interval: 60
  #   - name: "workstation"
  #     base_url: "http://192.168.1.100:1234"
  #     timeout: 300
  #     refresh_interval: 60
  #   - name: "server"
  #     base_url: "http://192.168.1.200:1234"
  #     timeout: 300
  #     refresh_interval: 60

router:
  model: "microsoft/phi-4"          # Router model (keep loaded in LM Studio)
  prefer_loaded_bonus: 50           # Score bonus for already-loaded models
  auto_load_models: true            # Allow loading new models automatically

performance_tracking:
  enabled: true                     # Learn from real inference metrics
  sample_size: 10                   # Track last N inferences per model

benchmarks:
  enabled: true                     # Include benchmark scores in routing context
  cache_ttl_hours: 24               # Refresh benchmark data every 24 hours
  auto_fetch_missing: true          # Fetch when encountering unknown models
  api_timeout: 30                   # Timeout for Artificial Analysis API

logging:
  level: "INFO"
  log_routing_decisions: true
